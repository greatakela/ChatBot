<p align="center">
  <img src="https://www.merchandisingplaza.co.uk/282130/2/Stickers-Star-Trek-STAR-TREK-Spock-Live-Long-Prosper-Sticker-l.jpg" 
       alt="Live Long and Prosper Sticker" 
       width="300">
</p>

# HW1 Retrieval-based чат бот

**Задание**: Необходимо разработать retrieval-based чат-бот, используя подход retrieval-based. Бот должен вести диалог как определенный персонаж сериала, имитируя стиль и манеру конкретного персонажа сериала. Важно учесть особенности речи и темы, которые поднимает персонаж, его типичные реакции.

## Данные
В качестве основы для чат-бота я взял скрипты к сериалам "Star Trek", которые загрузил из репозитория по [ссылке](https://github.com/varenc/star_trek_transcript_search), в часности реплики Мистера Спока, члена экипажа корабля, ученого с планеты Вулкан.

Данные обрабатываются следующим образом:
- очистка скрипта
- отбор реплик персонажа в качестве ответов. Именно из этих реплик будет выбирать бот свой ответ на высказывание пользователя.
- выделение предшествующей фразы как вопроса. Если это фраза первая в сцене, то это поле будет пустым.
- отбор предыдущих реплик как контекста диалога. Если фраза первая в сцене, то контекст также будет пустым. Контекст - идущие подряд предложения.

После обработки получилось около 5800 реплик персонажа, которые могут быть использованы ботом в диалоге. Затем я подготовил датасеты для обучения модели ранжирования и отбора кандидатов, и модели переранижирования кандидатов.

Код в ноутбуке [GNLP_HW1-data_prep.ipynb](https://github.com/greatakela/ChatBot/blob/main/Notebooks/GNLP_HW1-data_prep.ipynb)

### Данные для обучения bi-encoder 
На основе обработанных данных готовятся данные для обучения bi-encoder. Поскольку я обучаю модель c использованием triplet loss, то данные преобразуются в тройки:
- ANCHOR - комбинация контекста и вопросов
- ANSWER - ответ на вопрос из скрипта
- WRONG_ANSWER - случайно подобранный ответ (реплики взяты из сериала Др. Хаус)

### Данные для обучения reranker
Эти же данные будут использованы для обучения модели-reranker для переранжирования вариантов ответа. Исходные данные будут приняты как правильные с лейблом 0 и будут дополнены репликами из других сериалов в качестве ответа и помечены лейблом 1. 

Данные для reranker включают контекст-вопрос-ответ, разделенные специальным токеном [SEP]. Подготовлено примерно 10 тыс. образцов для обучения, разбивка по классам 50/50.

## Архитектура чат-бота

Схематично процесс работы чат-бота представлен на рисунке ниже.

![image](https://github.com/greatakela/ChatBot/blob/main/static/ArchBot.png)


**База данных реплик** включает векторизованные при помощи модели [обученного_энкодера](https://huggingface.co/greatakela/gnlp_hw1_encoder) скрипты, включающие контекст и вопрос. 

Отбор реплик из базы данных будет проводиться в 2 этапа:
- отбор похожих по косинусной близости контекст-вопрос из созданной векторной базы данных. В результате отбираются кандидаты с максимальной оценкой похожести на пользовательский контекст-вопрос.
- классификация моделью-переранжировщиком полученных кандидатов на предмет того, является ли подобранный ответ продолжением вопроса с контекстом или нет. Среди кандидатов отбираются те, которые были классифицированы как относящиеся к классу 0 (ответ является продолжением) и ранжируются по оценке уверенности модели в отнесении образца к нужному классу. Если же все реплики были классифицированы как относящиеся к 1 классу (ответ не является продолжением вопроса и контекста), то в ответ подается топ-1 из отобранных по косинусной близости.

**Классфикатор намерений** взят готовый из библиотеки [DialogTag](https://pypi.org/project/DialogTag/) и использовался как для разметки исходных данных, так и получаемого от пользователя высказывания. на основе полученного намерения фильтруется набор отобранных кандидатов. Также тег с намерением добавляется в эмбединги, которые использует bi-encoder.

В основе модели  **bi-encoder** ```distilroberta-base ```, обученная на описанных раннее данных в виде триплетов. Для обучения я воспользовался библиотекой sentence transformers. Обучение основано на Triplet Loss Function, которая минимизиурет расстояние между якорным предложением и правильным ответом и максимизирует между якорным и неправильным ответом. 

Оценивается модель по точности (accuracy) выявления случаев, когда близость между якорным текстом и правильным ответом больше, чем между якорным и нейтральным. Необученная ```distilroberta-base``` давала 58%, после обучения метрика стала на уровне 98%

<p align=center> <img src="https://github.com/greatakela/ChatBot/blob/main/static/evaluator_val.PNG" width="49.5%"> </p>

Код обучения находится в [ноутбуке](https://github.com/greatakela/ChatBot/blob/main/Notebooks/GNLP_HW1-bi_encoder_model_train.ipynb). Модель загружена в мой репозиторий на Hugging Face ([ссылка](https://huggingface.co/greatakela/gnlp_hw1_encoder)) и уже оттуда будет использоваться в инференсе.

В основе модели **re-ranker** ```bert-base-uncased```, обученная на подготовленных ранее данных. Классификация оценивалась при помощи accuracy. Ниже распечатки графиков обучения.

<img src="https://github.com/greatakela/ChatBot/blob/main/static/W%26B%20Chart%203_11_2025%2C%202_38_44%20PM.png" width="49.5%"> <img src="https://github.com/greatakela/ChatBot/blob/main/static/W%26B%20Chart%203_11_2025%2C%202_39_05%20PM.png" width="49.5%">

Получены неплохие результаты, финальная точность модели доходит до 95%. Видно, что после второй эпохи модель уже переобучилась.

Модель находится на Hugging Face ([ссылка](https://huggingface.co/greatakela/gnlp_hw1_reranker)) и уже оттуда будет использоваться в инференсе.

## Выводы по результатам модели:

Данные результаты указывают на высокую эффективность модели в процессе обучения и валидации. Стабильная точность порядка 95% на валидационном наборе данных может свидетельствовать о следующем:
1.	Переобучение: модель идеально подстроилась под тренировочные данные, что может привести к снижению ее способности к обобщению на новых данных. Однако стабильно высокая точность на валидационном наборе может говорить о том, что данные хорошо представляют общую закономерность, которую модель смогла выучить.
2.	Качество данных: возможно, датасет не достаточно разнообразен или слишком мал, что позволяет модели легко достичь высокой точности. В таком случае, важно обеспечить большую вариативность и объем обучающих данных.
Я склонен придерживаться второго мнения, т.к. итоговый набор данных действительно небольшой. 

## Структура репозитория

```bash
│   README.md - отчет по ДЗ 1
│   requirements.txt
│   __init__.py
│   retrieval_bot.py - основной файл с кодом инференса
│   utilities.py - вспомогательные функции
│   app.py - для запуска UI c flask
│
├───Notebooks - ноутбуки с подготовкой данных и обучением моделей
├───templates - оформление веб-интерфейса
│       chat.html
├───static - оформление веб-интерфейса
│       style.css
├───data
│       spock_dujour.pkl - сценарии при низких оценках похожести
│       spock_lines_vectorized.pkl - векторизованная база данных контекст-вопрос
│       spock_lines.pkl - исходные данные
│       spock_lines_reranker.pkl - исходные данные для переранжировщика
```

## Реализация web-сервиса

Чат реализован на основе Flask, запускается скриптом ```app.py```, который выстраивает графический интерфейс, создает инстант класса ChatBot, загружает файлы и модели. 

Для установки проекта нужно склонировать репозиторий ```https://github.com/greatakela/ChatBot.git```, создать среду, затем сделать установку ```pip install -r requirements.txt```. Чат-бот запускается командой ```python app.py```, и открывается в локальном окне браузера на ```http://127.0.0.1:5000```.

## Оценка качества чат-бота
Чат-бот должен оцениваться по релевантности реплик в контексте диалога, поэтому здесь основной все-таки будет пользовательская оценка. 

Я попробовал посмотреть, как будет отвечать чат-бот при применении разных видов энкодеров:
- [sentence-transformers/all-mpnet-base-v2]() - готовый обученный энкодер
- [sentence-transformers/LaBSE](https:/huggingface.co/sentence-transformers/LaBSE)- готовый обученный энкодер
- [greatakela/gnlp_hw1_encoder](https://huggingface.co/greatakela/gnlp_hw1_encoder) - энкодер, который обучила на данных, описанных выше для bi-encoder

Я выбрал несколько реплик и посмотрел, как работает retrieval на основе разных энкодеров.

| **Incoming** | **Greetings, Mr. Spock.** | **What is the logical course of action?** | **Explain your reasoning.** | **What do you think of Captain Kirk?** |
| :---: | :---: | :---: | :---: | :---: |
| **sentence-transformers/all-mpnet-base-v2** | Live long and prosper. | Logic is the beginning of wisdom, not the end. | Once you have eliminated the impossible, whatever remains, however improbable, must be the truth. | Captain, you almost make me believe in luck. |
| **sentence-transformers/LaBSE** | Greetings. How may I assist in your endeavors? | It would be illogical to assume that all conditions remain stable. | The universe is vast and full of wonders. It is logical to explore them. | Without followers, evil cannot spread. |
| **greatakela/gnlp_hw1_encoder** | I assume this greeting is a social convention rather than a necessity? | The needs of the many outweigh the needs of the few. | Superior ability breeds superior ambition. | I fail to comprehend your indignation, sir. I have simply made the logical deduction that you are a liar. |

Интересно, что при использовании эмбедингов от модели, обученных на данных по Споку, оценки похожести стали выше, чем при использовании более общих эмбедингов. 

Среди выше приведенных примеров сложно сказать, какой лучше. Я оставил обученную на даннаом датасете модель и добавил ограничение на намеренье и минимальный уровень похожести ответа перед передачей данных в переанжировщик, чтобы добавить немного детерминированности в диалог.

# Заключение
На основе проведенного анализа можно сделать вывод о высокой эффективности разработанной модели для задачи автоматизированного чат-бота. Однако для обеспечения более глубокого понимания ее способностей и ограничений необходимо провести дополнительные эксперименты, включая тестирование на более разнообразном и объемном наборе данных, а также оценку способности модели к обобщению на новых примерах.


